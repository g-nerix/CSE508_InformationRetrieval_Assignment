{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "582aec4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m      7\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bdaee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(content):\n",
    "    \"\"\"\n",
    "    Extracts the contents between <TITLE>...</TITLE> and <TEXT>...</TEXT>\n",
    "    tags and returns the concatenated data.\n",
    "    \"\"\"\n",
    "    content = re.sub('\\n', ' ', content)\n",
    "\n",
    "    title = re.search('<TITLE>(.*?)</TITLE>', content).group(1)\n",
    "\n",
    "    text = re.search('<TEXT>(.*?)</TEXT>', content).group(1)\n",
    "\n",
    "    data = title + \" \" + text\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfd2250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(directory = \"CSE508_Winter2023_Dataset\"):\n",
    "    data = {}\n",
    "    for filename in os.listdir(directory):    \n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r') as f:\n",
    "            file_contents = f.read()\n",
    "        data[filename] = file_contents\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40fd6c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File count =  1400\n"
     ]
    }
   ],
   "source": [
    "data = read_file()\n",
    "print(\"File count = \",len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dcb30d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(data):\n",
    "    extracted_data = {}\n",
    "    for i in data.keys():\n",
    "        extracted_data[i] = extract(data[i])\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f770bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = extract_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3db24f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e0775ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, directory_path):\n",
    "    \"\"\"\n",
    "    Saves data from a dictionary to files, where the keys are the file names and the values are the contents of the files.\n",
    "    \"\"\"\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "    for filename, file_contents in data.items():\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        with open(file_path, 'w') as f:\n",
    "            f.write(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab1570fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"CSE508_Winter2023_Dataset_Processed\"\n",
    "save_data(data,directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1da4729",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(data.items()),columns = ['file_name','raw_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccc6d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TEXT_LOWER']=df['raw_data'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffe5d865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>raw_data</th>\n",
       "      <th>TEXT_LOWER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cranfield0001</td>\n",
       "      <td>experimental investigation of the aerodynamic...</td>\n",
       "      <td>experimental investigation of the aerodynamic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cranfield0002</td>\n",
       "      <td>simple shear flow past a flat plate in an inc...</td>\n",
       "      <td>simple shear flow past a flat plate in an inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cranfield0003</td>\n",
       "      <td>the boundary layer in simple shear flow past ...</td>\n",
       "      <td>the boundary layer in simple shear flow past ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cranfield0004</td>\n",
       "      <td>approximate solutions of the incompressible l...</td>\n",
       "      <td>approximate solutions of the incompressible l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cranfield0005</td>\n",
       "      <td>one-dimensional transient heat conduction int...</td>\n",
       "      <td>one-dimensional transient heat conduction int...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_name                                           raw_data  \\\n",
       "0  cranfield0001   experimental investigation of the aerodynamic...   \n",
       "1  cranfield0002   simple shear flow past a flat plate in an inc...   \n",
       "2  cranfield0003   the boundary layer in simple shear flow past ...   \n",
       "3  cranfield0004   approximate solutions of the incompressible l...   \n",
       "4  cranfield0005   one-dimensional transient heat conduction int...   \n",
       "\n",
       "                                          TEXT_LOWER  \n",
       "0   experimental investigation of the aerodynamic...  \n",
       "1   simple shear flow past a flat plate in an inc...  \n",
       "2   the boundary layer in simple shear flow past ...  \n",
       "3   approximate solutions of the incompressible l...  \n",
       "4   one-dimensional transient heat conduction int...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce1c54be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenization\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEXT_LOWER\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mword_tokenize)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "df[\"tokenization\"] = df[\"TEXT_LOWER\"].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f790534c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>raw_data</th>\n",
       "      <th>TEXT_LOWER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cranfield0001</td>\n",
       "      <td>experimental investigation of the aerodynamic...</td>\n",
       "      <td>experimental investigation of the aerodynamic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cranfield0002</td>\n",
       "      <td>simple shear flow past a flat plate in an inc...</td>\n",
       "      <td>simple shear flow past a flat plate in an inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cranfield0003</td>\n",
       "      <td>the boundary layer in simple shear flow past ...</td>\n",
       "      <td>the boundary layer in simple shear flow past ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cranfield0004</td>\n",
       "      <td>approximate solutions of the incompressible l...</td>\n",
       "      <td>approximate solutions of the incompressible l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cranfield0005</td>\n",
       "      <td>one-dimensional transient heat conduction int...</td>\n",
       "      <td>one-dimensional transient heat conduction int...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_name                                           raw_data  \\\n",
       "0  cranfield0001   experimental investigation of the aerodynamic...   \n",
       "1  cranfield0002   simple shear flow past a flat plate in an inc...   \n",
       "2  cranfield0003   the boundary layer in simple shear flow past ...   \n",
       "3  cranfield0004   approximate solutions of the incompressible l...   \n",
       "4  cranfield0005   one-dimensional transient heat conduction int...   \n",
       "\n",
       "                                          TEXT_LOWER  \n",
       "0   experimental investigation of the aerodynamic...  \n",
       "1   simple shear flow past a flat plate in an inc...  \n",
       "2   the boundary layer in simple shear flow past ...  \n",
       "3   approximate solutions of the incompressible l...  \n",
       "4   one-dimensional transient heat conduction int...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51c31d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    tokens_clean = [token for token in tokens if token not in stop_words]\n",
    "    return tokens_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "786e1320",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tokenization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tokenization'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremove_stop_word\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(remove_stop_words)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tokenization'"
     ]
    }
   ],
   "source": [
    "df[\"remove_stop_word\"] = df[\"tokenization\"].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa418aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb5e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(tokens):\n",
    "    punctuation = string.punctuation\n",
    "    lst = []\n",
    "    for i in range(len(tokens)):\n",
    "        temp = \"\"\n",
    "        for j in tokens[i]:\n",
    "            if j not in punctuation:\n",
    "                temp+=j\n",
    "        tokens[i] = temp\n",
    "    # print(tokens)\n",
    "    # token_clean = [token for token in tokens if token not in punctuation]\n",
    "    #     # temp = \"\"\n",
    "    # print(token_clean)\n",
    "        # for t in token_clean:\n",
    "        #     temp+=t\n",
    "        # print(temp)\n",
    "        # lst.append(temp)\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0031e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"remove_punctuation\"] = df[\"remove_stop_word\"].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06555134",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba99c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_space(tokens):\n",
    "    token_clean = [token for token in tokens if token.strip()]\n",
    "    return token_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add46084",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"remove_space\"] = df[\"remove_punctuation\"].apply(remove_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d3280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8db64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = {}\n",
    "count = 0\n",
    "for i in df[\"remove_space\"]:\n",
    "    # df['id'].loc[data.index[0]]\n",
    "    temp = \"\"\n",
    "    count+=1\n",
    "    for j in i:\n",
    "        temp+=j+\" \"\n",
    "    data_new[df['file_name'].loc[df.index[count-1]]] = temp\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d7849",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"CSE508_Winter2023_Dataset_Processed_new\"\n",
    "save_data(data_new,directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89486a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "73bd8060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['s']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punctuation(\"'s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e9ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4bac288aa39488c199db7c1d2d3a4aa1846c8b7d5157a5261bbf38d0b7c4ca41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
